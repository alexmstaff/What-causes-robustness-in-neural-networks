# Calculating the robustness of neural networks

The code for this repository measures several neural networks' robustness to adversarial examples. It uses research and code of several papers listed below: 

* Akhilan Boopathy, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu and Luca Daniel, "CNN-Cert: An Efficient Framework for Certifying Robustness of Convolutional Neural Networks", AAAI 2019.
Their code can be found at https://github.com/IBM/CNN-Cert

* Nicholas Carlini and David Wagner, "Towards Evaluating the Robustness of Neural Networks", IEEE Symposium on Security & Privacy, 2017.
Their code can be found at https://github.com/carlini/nn_robust_attacks

* Chen, J., Jordan, M.I., Wainwright, M.J., 2019. Hopskipjumpattack: A query-efficient
decision-based attack.
Their code can be found at https://github.com/Jianbo-Lab/HSJA/.

* Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A., 2017. Towards deep learning
models resistant to adversarial attacks.
Their code can be found at https://github.com/MadryLab/cifar10_challenge.

* Dong, Y., Liao, F., Pang, T., Su, H., Zhu, J., Hu, X., Li, J., 2018. Boosting adversarial
attacks with momentum, in: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition.

## Pre-requisites
This repository was used on Python version 3.7.2 with the libraries found in /idun/requirements.txt
